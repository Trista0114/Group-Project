[
  {
    "objectID": "Teams/Proposal/proposal.html",
    "href": "Teams/Proposal/proposal.html",
    "title": "Green Spaces and Real Estate: Exploring the Influence of Environmental Factors on Housing Prices in Busan",
    "section": "",
    "text": "Generate by ChatGPT\n\n\n\n1 Background\nIn recent years, there has been a growing awareness of the benefits of living near environmental amenities, as more research uncovers the impact of neighborhood environments on both physical and mental health[1][2][3][4].\nStudies have shown that factors such as local built environment, proximity to environmental and built amenities and neighbourhood demographics, can significantly influence well-being. Specifically, residing near green spaces has been found to have a positive effect on mental health and encourages physical activity. Correspondingly, some studies have also found that environmental amenities could play a key role in influencing property prices [4], with growing recognition of the importance of green spaces. In Korea for instance, there is a gradual shift towards preference for residential areas near green spaces [5]. However, few studies have examined the relationship between proximity to green amenities and housing prices in Korea. \n\n\n2 Objectives\nThe primary objective of this project is to investigate how various factors, with a greater focus on the environmental amenities and the built environment factors, influences property prices in Korea. The project seeks to understand whether properties located closer to green spaces and other amenities tend to have higher market values. The secondary objective is to explore the interaction between green amenities, the built environment, and other factors, such as neighborhood demographics and seasonality, in shaping property prices.\nThe results from this project will help us gain insight on the factors driving property values in urban settings. As urbanization continues, the demand for green spaces and well-planned environments will likely increase. Understanding the connection between environmental amenities and local built environment and property prices could aid in creating more sustainable and livable urban areas, by offering insights for policymakers, estate developers and urban planners to make data-driven decisions on developing the city’s infrastructure and green spaces.\n\n\n3 Dataset\nTo achieve our objectives, we obtained a dataset used in a research paper to investigate variables influencing property prices in Korea. The dataset comprises of comprehensive information on the property prices in the Busan Metropolitan City of South Korea for transactions in 2018 to 2019 and various factors that influence property prices including characteristics of the property, distance to environmental amenities and local built environments, local demographic characteristics, and season in which the transaction occurred. \n\n\n4 Methodology & Approach\nWe will analyse and visualise the relationships between different factors and the housing prices in Busan by creating interactive R shiny applications.  Users will also be able to analyse the data based on their interests and draw their own insights and conclusions regarding the housing prices in Busan.\nWe will create 4 sub-modules to enable univariate analyses, bivariate analyses and multivariate analyses.\n\n\n5 Prototype Sketches\nThe following sketches were drawn to conceptualise our thoughts and discussions on the layout of the Shiny app along with its various functions. \n\n5.1 Univariate Analysis5.2 Bivariate Analysis5.3 Multivariate Analysis\n\n\nIn this sub-module, we aim to provide tools for analyzing individual variables. The interface includes summary statistics (mean, median, min, max, standard deviation), controls for variable selection and filtering, and a distribution visualization combining boxplot and half eye plot to display the selected variable’s characteristics. \n\n\n\nThis sub-module provides tools for examining relationships between pairs of variables. The interface features a correlation heatmap on the left showing strength of relationships between multiple variables, and a scatterplot on the right displaying the relationship between two selected variables. Controls allow users to select which continuous variables to include in the analysis for both visualization methods. \n\n\n\nThis panel offers tools for analyzing multiple variables simultaneously. It includes controls for selecting variables for clustering analysis, a dendrogram visualization showing hierarchical clustering results, and a parallel coordinate plot displaying relationships across multiple dimensions. \n\nThis panel provides radar plot functionality for multivariate visualization. Controls allow users to select encoded continuous variables and filter by a categorical variable. The radar plot displays multiple dimensions simultaneously with different colors representing different categories.\n\n\n\n\n\n\n6 R packages\nThe R packages we will be using for the project include: \n\nTidyverse for manipulating the data \n\n\n\nggstatplot for visualising statistcal analysis \n\n\n\nGGally / ggpcp / ggparallel for creating parallel coordinate plot \n\n\n\nggdist for visualising distributions\n\n\n\ncorrplot for correlation heatmap \n\n\n\nGgESDA / ggradar / fmsb for radar chart \n\n\n\n7 Project Timeline\n\n\n\n8 Reference\n[1]Yim, D. H., & Kwon, Y. (2021). Does young adults’ neighborhood environment affect their depressive mood? Insights from the 2019 Korean community health survey. International Journal of Environmental Research and Public Health, 18(3), 1269.\n[2]Choi, K. A., & Rezaei, M. (2022). Assessing the Correlation between Neighborhood Green Areas and the Perceived Mental Health of Residents in Metropolitan Areas. Iranian journal of public health, 51(9), 2027.\n\n[3]Han, M. J. N., & Kim, M. J. (2019). Green environments and happiness level in housing areas toward a sustainable life. Sustainability, 11(17), 4768.\n[4]An, S., Jang, H., Kim, H., Song, Y., & Ahn, K. (2023). Assessment of street-level greenness and its association with housing prices in a metropolitan area. Scientific reports, 13(1), 22577.\n[5]Korea Bizwire. (2023, July 8). Changing apartment preferences: Green spaces and comfort take priority in Korea’s housing market."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html",
    "href": "Teams/Minutes/Meeting_Minutes3.html",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 22/03/2025 5.00pm – 6.00pm\nMeeting Agenda:\n\nProject Modules & Shiny app Task allocations (updated base on prof reply)\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-1-project-modules-shiny-app-task-allocations",
    "href": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-1-project-modules-shiny-app-task-allocations",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "Agenda Item 1: Project Modules & Shiny app Task allocations",
    "text": "Agenda Item 1: Project Modules & Shiny app Task allocations\nFollowing Prof’s feedback on our proposal, the team decided to re-consider the modules to be created for the project. Prof recommended the team to split the modules according to (1) EDA & CDA, (2) Explanatory Modelling and (3) Predictive Modelling.  \nJia Rong said that the modules the team has previously proposed are all on EDA & CDA. Hence the team should select a few of the previous ideas and work on it as 1 module for EDA & CDA. For example, the team could do the boxplot and smoothed histogram for EDA, the scatterplot and the correlation heatmap as CDA. In addition, boxplot for ANOVA can be added for CDA. \nFor explanatory modelling, Jia Rong suggested doing multiple linear regression and perhaps latent class analysis, with reference to senior’s work. As for predictive modelling, Jia Rong suggested trying out random forest and decision tree. \nPeng-Wei suggested that for the random forest part, we can provide an interactive adjustment interface for parameters (such as the number of trees , the number of variables), and instantly displays the model’s prediction performance (such as Accuracy, RMSE, MAE, etc.) in the Shiny app. As for the decision Tree, we can make user adjust parameters such as tree depth and splitting method on the interface, and displaying auxiliary analysis charts such as predicted vs. actual values, residual plots, etc. \nXiao Han suggested using tidyverse, car, and lmtest packages for multiple linear regression analysis with ggplot2 for regression diagnostic plots, while implementing poLCA, mclust, and flexmix packages for latent class analysis since you have previous experience with this technique. She also recommended creating interactive elements using shiny, shinydashboard, and plotly to allow users to dynamically select variables and adjust model parameters."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-2-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-2-follow-up-action",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "Agenda Item 2: Follow-up Action",
    "text": "Agenda Item 2: Follow-up Action\nJia Rong will work on the EDA + CDA submodule. \nPeng-Wei will work on the predictive modelling module. \nXiao Han will work on the explanatory modelling module. \nAll team members agreed to finish their modules by 29th March and have another meeting to work on the poster and the user guide."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html",
    "href": "Teams/Minutes/Meeting_Minutes1.html",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Chen.PengWei, Moo.JiaRong, Zhang.XiaoHan\nDate: 23/02/2025 4.00pm – 05.30pm\nMeeting Agenda:\n\nProject Topic/Dataset\n\nProject Timeline\n\nProject Proposal\n\nWebsite Design Method\n\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-1-project-topicdataset",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-1-project-topicdataset",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 1: Project Topic/Dataset",
    "text": "Agenda Item 1: Project Topic/Dataset\nPeng-Wei suggested using a traffic dataset from Taiwan, which has details on the weather and location of the accidents.  However, as the dataset is in Chinese, the group decided that it will take too much time to encode the data in English. \nXiao Han suggested working on a dataset about problematic internet use in children. However, this dataset requires a lot of encoding and might be more suitable for machine learning. \nJia Rong suggested using the Seoul Bike Sharing Demand dataset and conducting EDA and a prediction model on it. However, the dataset is a bit old (2017–2018). \nXiaohan mentioned that the global traffic accident dataset could be used, but Jia Rong thinks that the dataset has too few feature columns. However, Peng-wei and Xiaohan believe that this dataset is more suitable for visualization as it includes latitude and longitude information. \nIn the end, all three agreed to choose the Seoul bicycle dataset and the global traffic accident dataset as alternatives and will consult the professor later."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-2-project-timeline",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-2-project-timeline",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 2: Project Timeline ",
    "text": "Agenda Item 2: Project Timeline \nThis timeline outlines key tasks, including proposal drafting, data cleaning, Shiny module development, poster creation, and user guide completion, ensuring a structured workflow."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-3-project-proposal",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-3-project-proposal",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 3: Project Proposal",
    "text": "Agenda Item 3: Project Proposal\n\nBelow is the proposal structure that we have agreed on: \n(1) Motivations  \n(2) Objective  \n(3) Dataset description  \n(4) Method – Data analysis approaches and the 3 different Shiny sub-modules each member will work on  \n(5) Prototype/storyboard drawing  \n(6) List of R packages that we will use   \n(7) Project timeline in Gantt Chart"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-4-website-design-method",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-4-website-design-method",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 4: Website design method",
    "text": "Agenda Item 4: Website design method\nFor the presentation style of the website, we want it to look professional and visually appealing, but without overly cute elements, as our content is related to transportation. \nOur initial idea is to place relevant images on the homepage and design a professional and aesthetically pleasing logo. \n\nAgenda Item 5: Follow-up Action\nPeng-Wei will send the email to professor to check which dataset (Seoul Bike Sharing & Global traffic) is more suitable to use in this project after this meeting. \n\n\n\n\n\n\n\nBefore the next meeting: \n\n\n\nAll team members will work on brainstorming the analysis approach, the type of visualizations we can create using our selected dataset and the potential topic for our project."
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html",
    "href": "Prototype/Predictive/Take-home_Ex3.html",
    "title": "Predictive Modeling",
    "section": "",
    "text": "In this take-home exercise, I will perform a predictive modeling analysis (Decision Tree and Random Forest) for the upcoming group project. Further result will be posted on here.\nA dataset used is from this research paper to investigate variables influencing property prices in Korea. The dataset comprises of comprehensive information on the property prices in the Busan Metropolitan City of South Korea for transactions in 2018 to 2019 and 27 variables that influence property prices including characteristics of the property, distance to environmental amenities and local built environments, local demographic characteristics, and season in which the transaction occurred."
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#select-variables",
    "href": "Prototype/Predictive/Take-home_Ex3.html#select-variables",
    "title": "Predictive Modeling",
    "section": "3.1 Select variables",
    "text": "3.1 Select variables\nRemove variables that are not useful for predictive modeling. Geographic data will not be used in this modeling.\n\nProperty_data &lt;- Property_data %&gt;% \n  select(!Longitude) %&gt;% \n  select(!Latitude)"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#create-summer-variable",
    "href": "Prototype/Predictive/Take-home_Ex3.html#create-summer-variable",
    "title": "Predictive Modeling",
    "section": "3.2 Create summer variable",
    "text": "3.2 Create summer variable\nI will create a Summer variable, where Summer = 1 if the property is recorded in summer (i.e., Spring = 0, Fall = 0, and Winter = 0). Since Spring, Fall, and Winter are already binary-encoded, we do not need to encode them further.\n\nProperty_data$summer &lt;- ifelse(Property_data$Spring == 0 & Property_data$Fall == 0 & Property_data$Winter == 0, 1, 0)\n\nNow check your data set again. There will be 27 variables with the new binary variable summer.\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 27\n$ `Property Prices`  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9…\n$ Size               &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.070…\n$ Floor              &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4…\n$ `Highest floor`    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11,…\n$ Units              &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8…\n$ Parking            &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.6…\n$ Heating            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year               &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 201…\n$ `Dist. Green`      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4…\n$ `Dist. Water`      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7…\n$ `Green Index`      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.8678…\n$ `Dist. Subway`     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5…\n$ `Bus Stop`         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, …\n$ `Dist. CBD`        &lt;dbl&gt; 19909.90, 19909.90, 19909.90, 19909.90, 19909.90, 1…\n$ `Top Univ.`        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `High School`      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ `Sex Ratio`        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 9…\n$ Population         &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 33…\n$ `Pop. Density`     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 1…\n$ `Higher Degree`    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 4…\n$ `Young Population` &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 2…\n$ `Median Age`       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.…\n$ `Old Population`   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5…\n$ Spring             &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n$ Fall               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Winter             &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, …\n$ summer             &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#rename-the-variable",
    "href": "Prototype/Predictive/Take-home_Ex3.html#rename-the-variable",
    "title": "Predictive Modeling",
    "section": "3.3 Rename the variable",
    "text": "3.3 Rename the variable\nIn R, it will be better for us using the column name without the blank.\n\nnames(Property_data) &lt;- gsub(\" \", \"_\", names(Property_data))"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#outlier",
    "href": "Prototype/Predictive/Take-home_Ex3.html#outlier",
    "title": "Predictive Modeling",
    "section": "4.1 Outlier",
    "text": "4.1 Outlier\nWhile decision trees are generally robust to outliers due to their non-parametric nature, it is recommended to check for outliers before training. Outliers may affect split point selection, especially when the feature range is large. In this analysis, we will inspect the data for outliers and decide on the appropriate action.\n\n\nshow the code\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\nfind_outliers &lt;- function(x) {\n  stats &lt;- boxplot.stats(x)$stats  \n  lower_bound &lt;- stats[1]             \n  upper_bound &lt;- stats[5]             \n  outliers &lt;- x[x &lt; lower_bound | x &gt; upper_bound]\n  return(outliers)\n}\n\nboxplot(numeric_data, main=\"Box Plot for Numerical Data\", las=2)\n\n\n\n\n\n\n\n\n\nIt is observed that the variable Dist_CBD has an extremely large range and a right-skewed distribution. To reduce skewness and stabilize the feature’s range for better model performance, we apply a log transformation using log(X+1).\n\nProperty_data$`Dist._CBD` &lt;- log(Property_data$`Dist._CBD` + 1)\n\nNow check outlier again.\n\n\nshow the code\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\nfind_outliers &lt;- function(x) {\n  stats &lt;- boxplot.stats(x)$stats  \n  lower_bound &lt;- stats[1]             \n  upper_bound &lt;- stats[5]             \n  outliers &lt;- x[x &lt; lower_bound | x &gt; upper_bound]\n  return(outliers)\n}\n\nboxplot(numeric_data, main=\"Box Plot for Numerical Data\", las=2)\n\n\n\n\n\n\n\n\n\nAfter checking the numerical variables, we found that most outliers come from the Population variable. Since Population may have a meaningful relationship with our dependent variable Property_Prices (e.g., higher population density might correlate with higher property prices), we will remain these outliers for now and evaluate their impact on the model later."
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#correlation",
    "href": "Prototype/Predictive/Take-home_Ex3.html#correlation",
    "title": "Predictive Modeling",
    "section": "4.2 Correlation",
    "text": "4.2 Correlation\nBefore building the decision tree, it is necessary to check the correlation between variables. However, we will allow users to select the variables themselves in Shiny. By default, the pre-cleaned variable will be selected.\n\n\nshow the code\ncutoff &lt;- 0.8\n\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\ncorr_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\nvar_names &lt;- names(numeric_data)\n\nfor (i in 1:(ncol(numeric_data) - 1)) {\n  for (j in (i + 1):ncol(numeric_data)) {\n    cor_val &lt;- corr_matrix[i, j]\n    if (abs(cor_val) &gt; cutoff) {\n      cat(\"correlation coefficient between\" , var_names[i], \"and\", var_names[j], \n round(cor_val, 3), \"\\n\")\n    }\n  }\n}\n\n\ncorrelation coefficient between Dist._Green and Dist._CBD -0.93 \ncorrelation coefficient between Top_Univ. and High_School 0.806 \n\n\nDue to multicollinearity, one feature from each pair may need to be dropped.\n\n4.2.1 Feature selection\nTo choose which feature we need to drop in each pair, do the simple decision tree to compare the variable’s importance level for the dependent variable.\n\nmodel &lt;- train(\n  Property_Prices ~ Dist._Green + Dist._CBD + Top_Univ. + High_School,\n  data = Property_data,\n  method = \"rpart\"\n)\nimportance &lt;- varImp(model)\nprint(importance)\n\nrpart variable importance\n\n            Overall\nDist._CBD    100.00\nTop_Univ.     26.03\nDist._Green   10.67\nHigh_School    0.00\n\n\nNow, drop the least important variable in each pair. We will drop “Dist._Green” and “High_School”.\n\nProperty_data_analysis &lt;- Property_data[, \n  !(names(Property_data) %in% c(\"Dist._Green\", \"High_School\"))\n]\n\nBefore we start building the model, check the data again.\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 27\n$ Property_Prices  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9.7…\n$ Size             &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.0700,…\n$ Floor            &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4, …\n$ Highest_floor    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11, 1…\n$ Units            &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8, …\n$ Parking          &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67,…\n$ Heating          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Year             &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,…\n$ Dist._Green      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4.6…\n$ Dist._Water      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7.0…\n$ Green_Index      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.867812…\n$ Dist._Subway     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5.6…\n$ Bus_Stop         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, 13…\n$ Dist._CBD        &lt;dbl&gt; 9.899023, 9.899023, 9.899023, 9.899023, 9.899023, 9.8…\n$ Top_Univ.        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ High_School      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Sex_Ratio        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 97.…\n$ Population       &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 3344…\n$ Pop._Density     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 163…\n$ Higher_Degree    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 46.…\n$ Young_Population &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 26.…\n$ Median_Age       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4,…\n$ Old_Population   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5.7…\n$ Spring           &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,…\n$ Fall             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ Winter           &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,…\n$ summer           &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#building-model",
    "href": "Prototype/Predictive/Take-home_Ex3.html#building-model",
    "title": "Predictive Modeling",
    "section": "5.1 Building model",
    "text": "5.1 Building model\nFirst, plit the dataset into training (80%) and testing (20%) sets. Using 1234 as the seed amount.\nIn this case, Property_Prices is a continuous variable, so method = “anova” is the appropriate choice.\n\nnames(Property_data_analysis) &lt;- gsub(\" \", \"_\", names(Property_data_analysis))\nset.seed(1234)\n\ntrainIndex &lt;- createDataPartition(Property_data_analysis$`Property_Prices`, p = 0.8, \n                                  list = FALSE, \n                                  times = 1)\n\ndf_train &lt;- Property_data_analysis[trainIndex,]\ndf_test &lt;- Property_data_analysis[-trainIndex,]\n\nTrained an initial regression tree using rpart with the following parameters: minsplit = 5, cp = 0.001, and maxdepth = 10. These settings allowed the tree to grow with some constraints to avoid overfitting.\n\nanova.model &lt;- function(min_split, complexity_parameter, max_depth) {\n  rpart(`Property_Prices` ~ ., \n        data = df_train , \n        method = \"anova\", \n        control = rpart.control(minsplit = min_split, \n                                cp = complexity_parameter, \n                                maxdepth = max_depth))\n  }\n\nfit_tree &lt;- anova.model(5, 0.001, 10)\n\n\nvisTree(fit_tree, edgesFontSize = 14, nodesFontSize = 16, width = \"100%\")"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#tuning-of-hyperparameters",
    "href": "Prototype/Predictive/Take-home_Ex3.html#tuning-of-hyperparameters",
    "title": "Predictive Modeling",
    "section": "5.2 Tuning of hyperparameters",
    "text": "5.2 Tuning of hyperparameters\nTo optimize the tree, here we use the complexity parameter (CP) table (cptable) to identify the CP value that minimized the cross-validation error (xerror).\n\nprintcp(fit_tree)\n\n\nRegression tree:\nrpart(formula = Property_Prices ~ ., data = df_train, method = \"anova\", \n    control = rpart.control(minsplit = min_split, cp = complexity_parameter, \n        maxdepth = max_depth))\n\nVariables actually used in tree construction:\n [1] Dist._CBD     Dist._Subway  Green_Index   Heating       Higher_Degree\n [6] Highest_floor Median_Age    Parking       Population    Sex_Ratio    \n[11] Size          Units         Year         \n\nRoot node error: 13564/42117 = 0.32205\n\nn= 42117 \n\n          CP nsplit rel error  xerror      xstd\n1  0.3670796      0   1.00000 1.00007 0.0072552\n2  0.1003165      1   0.63292 0.63376 0.0046167\n3  0.0845250      2   0.53260 0.53303 0.0039374\n4  0.0281388      3   0.44808 0.44856 0.0033575\n5  0.0260674      4   0.41994 0.42040 0.0031686\n6  0.0257959      5   0.39387 0.39161 0.0029997\n7  0.0190744      6   0.36808 0.36859 0.0028409\n8  0.0128092      7   0.34900 0.35003 0.0027295\n9  0.0110859      8   0.33619 0.33717 0.0026830\n10 0.0082922      9   0.32511 0.32613 0.0026348\n11 0.0079784     10   0.31682 0.31867 0.0026345\n12 0.0070377     11   0.30884 0.30873 0.0025974\n13 0.0062992     12   0.30180 0.30143 0.0025070\n14 0.0053719     13   0.29550 0.29666 0.0024660\n15 0.0053494     14   0.29013 0.29334 0.0024275\n16 0.0048172     15   0.28478 0.28723 0.0023723\n17 0.0044387     16   0.27996 0.27925 0.0022879\n18 0.0042845     17   0.27552 0.27605 0.0022688\n19 0.0042792     18   0.27124 0.27358 0.0022510\n20 0.0039115     19   0.26696 0.26885 0.0022247\n21 0.0032249     20   0.26305 0.26187 0.0022090\n22 0.0029020     21   0.25982 0.25732 0.0021862\n23 0.0028710     22   0.25692 0.25509 0.0021776\n24 0.0027361     23   0.25405 0.25000 0.0021617\n25 0.0026756     24   0.25131 0.24830 0.0021576\n26 0.0026218     25   0.24864 0.24690 0.0021617\n27 0.0025135     27   0.24339 0.24636 0.0021602\n28 0.0024512     28   0.24088 0.24443 0.0021524\n29 0.0022157     29   0.23843 0.23954 0.0021279\n30 0.0021056     30   0.23621 0.23869 0.0021166\n31 0.0020036     31   0.23411 0.23586 0.0020999\n32 0.0018931     33   0.23010 0.23215 0.0020393\n33 0.0018549     34   0.22821 0.23027 0.0020330\n34 0.0018325     36   0.22450 0.22786 0.0020233\n35 0.0018306     37   0.22267 0.22708 0.0020129\n36 0.0018065     38   0.22084 0.22524 0.0019517\n37 0.0018002     39   0.21903 0.22513 0.0019519\n38 0.0016511     40   0.21723 0.21993 0.0019180\n39 0.0016419     41   0.21558 0.21705 0.0019060\n40 0.0016414     42   0.21394 0.21705 0.0019060\n41 0.0015938     43   0.21229 0.21488 0.0018984\n42 0.0015292     44   0.21070 0.21240 0.0018866\n43 0.0015192     45   0.20917 0.20883 0.0018702\n44 0.0014909     48   0.20461 0.20662 0.0018579\n45 0.0014809     49   0.20312 0.20324 0.0018475\n46 0.0014746     50   0.20164 0.20312 0.0018401\n47 0.0014727     51   0.20017 0.20266 0.0018385\n48 0.0014677     52   0.19869 0.20237 0.0018365\n49 0.0014513     53   0.19723 0.20158 0.0018291\n50 0.0014251     54   0.19578 0.19997 0.0018185\n51 0.0013960     55   0.19435 0.19798 0.0018134\n52 0.0013757     56   0.19295 0.19575 0.0017979\n53 0.0013575     57   0.19158 0.19469 0.0017889\n54 0.0013406     58   0.19022 0.19306 0.0017787\n55 0.0013342     59   0.18888 0.19159 0.0017520\n56 0.0013215     61   0.18621 0.19038 0.0017474\n57 0.0012404     62   0.18489 0.18765 0.0017291\n58 0.0012187     64   0.18241 0.18417 0.0017182\n59 0.0011913     65   0.18119 0.18311 0.0017111\n60 0.0011332     66   0.18000 0.18082 0.0016998\n61 0.0011152     67   0.17887 0.17901 0.0016876\n62 0.0010916     68   0.17775 0.17727 0.0016730\n63 0.0010737     69   0.17666 0.17685 0.0016702\n64 0.0010453     70   0.17559 0.17576 0.0016629\n65 0.0010384     71   0.17454 0.17373 0.0016532\n66 0.0010367     72   0.17350 0.17315 0.0016506\n67 0.0010057     73   0.17247 0.17256 0.0016486\n68 0.0010000     74   0.17146 0.17141 0.0016420\n\n\n\nbestcp &lt;- fit_tree$cptable[which.min(fit_tree$cptable[,\"xerror\"]),\"CP\"]\npruned_tree &lt;- prune(fit_tree, cp = bestcp)\nvisTree(pruned_tree, edgesFontSize = 14, nodesFontSize = 16, width = \"100%\")"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation",
    "href": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation",
    "title": "Predictive Modeling",
    "section": "5.3 Model Evaluation",
    "text": "5.3 Model Evaluation\n\n\n\n\n\n\nchecking overfitting\n\n\n\nThe training set R-squared (0.8285) is slightly higher than the test set (0.8239), the difference is about 0.47%, which is very small. In terms of MSE, the test set MSE is slightly higher than the training set, but the difference (0.00167) is also small. In conclusion, the performance of the training and test sets is very close, the model has almost no overfitting, and the generalization ability is good.\n\n\nR-squared is only 0.8239, and the model’s prediction ability is weak. It is recommended to use random forest or further adjust the parameters.\n\n\nshow the code\ntrain_pred &lt;- predict(pruned_tree, newdata = df_train)\ntest_pred &lt;- predict(pruned_tree, newdata = df_test)\n\ntrain_mse &lt;- mean((train_pred - df_train$Property_Prices)^2)\ntest_mse &lt;- mean((test_pred - df_test$Property_Prices)^2)\ntrain_r2 &lt;- 1 - sum((train_pred - df_train$Property_Prices)^2) / \n              sum((df_train$Property_Prices - mean(df_train$Property_Prices))^2)\ntest_r2 &lt;- 1 - sum((test_pred - df_test$Property_Prices)^2) / \n             sum((df_test$Property_Prices - mean(df_test$Property_Prices))^2)\n\ncat(sprintf(\"Train MSE: %.6f\", train_mse))\n\n\nTrain MSE: 0.055219\n\n\nshow the code\ncat(sprintf(\"Test MSE: %.6f\", test_mse))\n\n\nTest MSE: 0.056887\n\n\nshow the code\ncat(sprintf(\"Train R-squared: %.6f\", train_r2))\n\n\nTrain R-squared: 0.828540\n\n\nshow the code\ncat(sprintf(\"Test R-squared: %.6f\", test_r2))\n\n\nTest R-squared: 0.823885\n\n\nThrough the cp table, select an optimal CP value to prune the tree to balance fitting and generalization capabilities. The CP value with the lowest xerror is usually chosen because this indicates that the model performs best on unseen data. ( can be put in shiny)\n\n\nshow the code\ncp_table &lt;- as.data.frame(pruned_tree$cptable)\nnames(cp_table) &lt;- c(\"CP\", \"nsplit\", \"rel_error\", \"xerror\", \"xstd\") \nhead(pruned_tree$cptable, 20)\n\n\n            CP nsplit rel error    xerror        xstd\n1  0.367079585      0 1.0000000 1.0000665 0.007255185\n2  0.100316546      1 0.6329204 0.6337603 0.004616708\n3  0.084525003      2 0.5326039 0.5330338 0.003937360\n4  0.028138831      3 0.4480789 0.4485562 0.003357522\n5  0.026067399      4 0.4199400 0.4203964 0.003168595\n6  0.025795914      5 0.3938726 0.3916060 0.002999738\n7  0.019074431      6 0.3680767 0.3685916 0.002840923\n8  0.012809171      7 0.3490023 0.3500296 0.002729456\n9  0.011085850      8 0.3361931 0.3371698 0.002682996\n10 0.008292161      9 0.3251073 0.3261289 0.002634750\n11 0.007978381     10 0.3168151 0.3186709 0.002634465\n12 0.007037659     11 0.3088367 0.3087277 0.002597356\n13 0.006299227     12 0.3017991 0.3014349 0.002506989\n14 0.005371917     13 0.2954998 0.2966615 0.002466042\n15 0.005349426     14 0.2901279 0.2933445 0.002427479\n16 0.004817197     15 0.2847785 0.2872339 0.002372311\n17 0.004438673     16 0.2799613 0.2792518 0.002287886\n18 0.004284460     17 0.2755226 0.2760482 0.002268796\n19 0.004279165     18 0.2712382 0.2735801 0.002250999\n20 0.003911487     19 0.2669590 0.2688509 0.002224722"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#visualization",
    "href": "Prototype/Predictive/Take-home_Ex3.html#visualization",
    "title": "Predictive Modeling",
    "section": "5.4 Visualization",
    "text": "5.4 Visualization\n\n5.4.1 Predicted vs Actual Plot\nThe points were generally close to the line, confirming the model’s reasonable predictive performance.\n\n\nshow the code\ntrain_pred &lt;- predict(pruned_tree, newdata = df_train)\ntrain_sse &lt;- sum((train_pred - df_train$Property_Prices)^2)\ntrain_sst &lt;- sum((df_train$Property_Prices - mean(df_train$Property_Prices))^2)\ntrain_r2 &lt;- 1 - train_sse / train_sst\ndf_test$Predicted &lt;- predict(pruned_tree, newdata = df_test)\n\ntree_scatter &lt;- ggplot(df_test, aes(x = Property_Prices, y = Predicted)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  \n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Predicted Property_Prices\",\n    title = paste0(\"R-squared (train): \", round(train_r2, 2))\n  ) +\n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8),\n    plot.title = element_text(size = 8)\n  )\ntree_scatter\n\n\n\n\n\n\n\n\n\nWe can also compare the actual and predicted data by using boxplot.\n\n\nshow the code\nplot_data &lt;- data.frame(\n  Value = c(df_test$Property_Prices, df_test$Predicted),\n  Type = rep(c(\"Actual\", \"Predicted\"), each = nrow(df_test))\n)\nggplot(plot_data, aes(x = Type, y = Value, fill = Type)) +\n  geom_boxplot() +\n  labs(x = NULL, y = \"Property Prices\", title = \"Boxplot of Actual vs Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Residual Plot\nThe residual plot showed that residuals were mostly centered around 0, with a slight left skew, indicating that the model occasionally underestimates property prices.\n\n\nshow the code\nresiduals &lt;- test_pred - df_test$Property_Prices\nplot_data &lt;- data.frame(Predicted = test_pred, Residuals = residuals)\nggplot(plot_data, aes(x = Predicted, y = Residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(title = \"Residuals vs Predicted\", x = \"Predicted Property Prices\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Feature Importance\nA bar plot of feature importance highlighted Size the most influential predictors of property prices, while features like Bus_Stop had small impact.\n\n\nshow the code\nvi &lt;- pruned_tree$variable.importance\nvi_df &lt;- data.frame(\n  Variable = names(vi),\n  Importance = as.numeric(vi)\n)\n\nggplot(vi_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_col(fill = \"lightblue\") +\n  geom_text(aes(label = round(Importance, 2)), hjust = -0.2, size = 3) +  \n  coord_flip() +\n  labs(x = NULL, y = \"Importance\", title = \"Variable Importance from Regression Tree Model\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#building-model-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#building-model-1",
    "title": "Predictive Modeling",
    "section": "6.1 Building Model",
    "text": "6.1 Building Model\nThe default is 8:2 for the training and testing data set. The user can choose the seed number and the ratio of training data.\n\nset.seed(1234)\n\ntrainIndex &lt;- createDataPartition(Property_data_analysis$`Property_Prices`, p = 0.8, list = FALSE,  times = 1)\n\ndf_train &lt;- Property_data_analysis[trainIndex,]\ndf_test &lt;- Property_data_analysis[-trainIndex,]\n\n\n\nfind the best setting\nregisterDoParallel(cores = 4)\n\ntrctrl_none &lt;- trainControl(method = \"none\")\ntrctrl_cv &lt;- trainControl(method = \"cv\", number = 5, verboseIter = TRUE)  trctrl_repeatedcv &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3, verboseIter = TRUE)  \ntrctrl_boot &lt;- trainControl(method = \"boot\", number = 5, verboseIter = TRUE)\n\n\ntune_grid &lt;- expand.grid( mtry = c(2, 4, 8),\n                          min.node.size = c(3, 5, 10), splitrule = c(\"variance\", \"extratrees\") )\n\n\nevaluate_model &lt;- function(tr_control, num_trees, importance_method) \n  { rf_model &lt;- train( Property_Prices ~ ., data = df_train, method = \"ranger\", trControl = tr_control, tuneGrid = tune_grid, num.trees = num_trees, importance = importance_method )\n\n\n  best_params &lt;- rf_model$bestTune cat(\"Cross-validation results:\\n\") \n  print(cv_results) cat(\"Best parameters:\", best_params$mtry, best_params$min.node.size, best_params$splitrule, \"\\n\")\n\npredictions &lt;- predict(rf_model, newdata = df_test) mse &lt;- mean((predictions - df_test$`Property_Prices`)^2)\n  sst &lt;- sum((df_test$Property_Prices - mean(df_test$`Property_Prices`))^2)\n  sse &lt;- sum((predictions - df_test$Property_Prices)^2) r_squared &lt;- 1 - sse / sst cat(\"Test MSE:\", mse, \"\\n\") cat(\"Test R-squared:\", r_squared, \"\\n\")\n\nreturn(list(model = rf_model, cv_results = cv_results, test_mse = mse, test_r_squared = r_squared)) }\n\n\n\ncat(\"\\n=== 5-fold CV, num.trees = 50, importance = impurity ===\\n\") result_cv_50_impurity &lt;- evaluate_model(trctrl_cv, 50, \"impurity\")\n\n\ncat(\"\\n=== Repeated CV (5-fold, 3 repeats), num.trees = 100, importance = permutation ===\\n\") result_repeatedcv_100_permutation &lt;- evaluate_model(trctrl_repeatedcv, 100, \"permutation\")\n\n\ncat(\"\\n=== Bootstrap, num.trees = 200, importance = impurity ===\\n\") result_boot_200_impurity &lt;- evaluate_model(trctrl_boot, 200, \"impurity\")\n\n\nresults_summary &lt;- data.frame( Method = c(\"5-fold CV\", \"Repeated CV (5-fold, 3 repeats)\", \"Bootstrap\"), Num_Trees = c(50, 100, 200), Importance = c(\"impurity\", \"permutation\", \"impurity\"), Test_MSE = c(result_cv_50_impurity$test_mse,\n               result_repeatedcv_100_permutation$test_mse, result_boot_200_impurity$test_mse),\n  Test_R_squared = c(result_cv_50_impurity$test_r_squared, result_repeatedcv_100_permutation$test_r_squared,\n                     result_boot_200_impurity$test_r_squared) ) print(results_summary)\n\nbest_method &lt;- results_summary[which.max(results_summary$Test_R_squared), ] cat(\"\\nBest analysis method:\\n\") print(best_method) '''\n\n\n\n# user can choose to set random seed  (default 1234)\n# user can choose the tuning method :none, cv, repeatedcv, boot(default)\n# user can choose splitrule: variance(default), extratrees, maxstat, beta\n# user can choose num of tree :5-200 (200 is default)\n# user can choose feature importance :\"impurity\", \"permutation\n\ntrctrl &lt;- trainControl(method = \"boot\", number = 5, verboseIter = TRUE)  # number of boot is default\n\n\ntune_grid &lt;- expand.grid(\n  mtry = 8,  # default\n  min.node.size = 3,  # default\n  splitrule = \"variance\"  \n)\n\n\nrf_model &lt;- train(\n  `Property_Prices` ~ ., \n  data = df_train,\n  method = \"ranger\", \n  trControl = trctrl,\n  tuneGrid = tune_grid,\n  num.trees = 200,  \n  importance = \"impurity\" \n)\n\n+ Resample1: mtry=8, min.node.size=3, splitrule=variance \n- Resample1: mtry=8, min.node.size=3, splitrule=variance \n+ Resample2: mtry=8, min.node.size=3, splitrule=variance \n- Resample2: mtry=8, min.node.size=3, splitrule=variance \n+ Resample3: mtry=8, min.node.size=3, splitrule=variance \n- Resample3: mtry=8, min.node.size=3, splitrule=variance \n+ Resample4: mtry=8, min.node.size=3, splitrule=variance \n- Resample4: mtry=8, min.node.size=3, splitrule=variance \n+ Resample5: mtry=8, min.node.size=3, splitrule=variance \n- Resample5: mtry=8, min.node.size=3, splitrule=variance \nAggregating results\nFitting final model on full training set"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation-1",
    "title": "Predictive Modeling",
    "section": "6.2 Model Evaluation",
    "text": "6.2 Model Evaluation\nThe high R-squared on the test set (0.97) indicates that the random forest model captures the underlying patterns in the data effectively, outperforming the regression tree model (test R-squared of 0.82).\nIf the training set performance (such as R-squared) is much higher than the test set, it may be overfitting. The difference is about 2%, showing slight overfitting, but not a big difference.\n\n\nshow the code\npredictions &lt;- predict(rf_model, newdata = df_test)\nmse &lt;- mean((predictions - df_test$`Property_Prices`)^2)\nr_squared &lt;- 1 - sum((predictions - df_test$`Property_Prices`)^2) / \n                 sum((df_test$`Property_Prices` - mean(df_test$`Property_Prices`))^2)\n\ntrain_predictions &lt;- predict(rf_model, newdata = df_train)\ntrain_mse &lt;- mean((train_predictions - df_train$`Property_Prices`)^2)\ntrain_sst &lt;- sum((df_train$`Property_Prices` - mean(df_train$`Property_Prices`))^2)\ntrain_sse &lt;- sum((train_predictions - df_train$`Property_Prices`)^2)\ntrain_r_squared &lt;- 1 - train_sse / train_sst\n\ncat(\"Test MSE:\", mse)\n\n\nTest MSE: 0.008759057\n\n\nshow the code\ncat(\"Test R-squared:\", r_squared)\n\n\nTest R-squared: 0.9728832\n\n\nshow the code\ncat(\"Train MSE:\", train_mse)\n\n\nTrain MSE: 0.002337131\n\n\nshow the code\ncat(\"Train R-squared:\", train_r_squared)\n\n\nTrain R-squared: 0.992743\n\n\nshow the code\ncat(\"Difference between test and train R-squared:\", train_r_squared - r_squared, \"\\n\")\n\n\nDifference between test and train R-squared: 0.01985979 \n\n\nOOB MSE is the error estimate of unseen data. If it is stable as the number of trees increases, the model has good generalization ability. In this figure, the OOB MSE stabilizes after 50 trees and does not increase significantly, indicating that the model is not overfitted. If it is overfitted, the OOB MSE may start to rise after a certain point.\nThe training set MSE is lower than the OOB and test sets, indicating that the model slightly overfits the training data, but the impact is not significant.\n\n\nshow the code\noob_errors &lt;- sapply(seq(10, 200, by = 10), function(ntree) {\n  model &lt;- ranger(`Property_Prices` ~ ., data = df_train, num.trees = ntree, importance = \"impurity\")\n  return(model$prediction.error)\n})\nplot(seq(10, 200, by = 10), oob_errors, type = \"l\", xlab = \"Number of Trees\", ylab = \"OOB MSE\", main = \"OOB Error vs Number of Trees\")\n\n\n\n\n\n\n\n\n\nshow the code\noob_mse &lt;- rf_model$finalModel$prediction.error\ncat(\"OOB MSE:\", oob_mse, \"\\n\")\n\n\nOOB MSE: 0.009510285"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#visualization-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#visualization-1",
    "title": "Predictive Modeling",
    "section": "6.3 Visualization",
    "text": "6.3 Visualization\n\n6.3.1 Predicted vs Actual Plot\nBased on the chart, the points were closely aligned with the line, confirming the model’s high predictive accuracy (test R-squared of 0.972). The residuals were mostly centered around 0 with minimal spread, indicating that the model’s predictions were highly accurate and lacked systematic bias.\n\n\nshow the code\ndf_test$Predicted &lt;- predict(rf_model, newdata = df_test)\n\nrf_scatter &lt;- ggplot(df_test, aes(x = Property_Prices, y = Predicted)) +\n  geom_point() +\n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Predicted Property_Prices\",\n    title = paste0(\"R-squared (train): \", round(rf_model$finalModel$r.squared, 2))\n  ) +\n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8),\n    plot.title = element_text(size = 8)\n  )\n\ndf_test$Residuals &lt;- df_test$Predicted - df_test$Property_Prices\n\nrf_residuals &lt;- ggplot(df_test, aes(x = Property_Prices, y = Residuals)) + \n  geom_point(color = \"blue3\") +\n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Residuals (Predicted - Actual)\"\n  ) + \n  geom_hline(yintercept = 0, color = \"red4\", linetype = \"dashed\", linewidth = 0.5) + \n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8)\n  )\n\np &lt;- rf_scatter + rf_residuals +\n  plot_annotation(\n    title = \"Scatterplot of Predicted vs. Actual Property_Prices\",\n    theme = theme(plot.title = element_text(size = 18))\n  )\n\n\np\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Feature Importance\nThe plot highlighted that Size, Parking, Highest_Floor, Year, and Units were the most influential predictors of property prices, while features like spring had negligible importance (scores close to 0).\n\n\nshow the code\nvi &lt;- varImp(rf_model)\nvi_df &lt;- as.data.frame(vi$importance)\nvi_df$Variable &lt;- rownames(vi_df)\n\nggplot(vi_df, aes(x = reorder(Variable, Overall), y = Overall)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = round(Overall, 2)), hjust = -0.2, size = 3) +  # \n  coord_flip() +\n  labs(x = NULL, y = \"Importance (Overall)\", title = \"Variable Importance from Random Forest Model\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "isss608 Group Project",
    "section": "",
    "text": "This is a home page for Group5.\n\nBuilding now.."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Prototype/EDA/Take-home_Ex03.html",
    "href": "Prototype/EDA/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "1 Background\nIn this take-home exercise, I will be prototyping a shiny module for my group project, which aims to investigate how various factors, with a greater focus on the environmental amenities and the built environment factors, influences property prices in Korea.\nTHe objectives of this exercise includes:\n\nTo evaluate and determine the necessary R packages needed for the Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and return the correct output as expected,\nTo determine the parameters and outputs that should be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nSpecifically, I will be working on a shiny module to perform Exploratory Data ANalysis (EDA) and Confirmatory Data Analysis (CDA).\n\n\n2 Getting Started\n\nLoading LibrariesImporting DatasetGlimpse dataMissing dataDuplicated Records\n\n\nWe will load the following packages:\n\ntidyverse to wrangle data\nSmartEDA to explore data\nggdist for visualising distributions and uncertainty\nparallelPlot for plotting parallel coordinates plot with histogram\nggstatsplot for correlation heat map\nggside for adding histograms to scatterplot\nplotly for interactive plots\nggpubr to add statistical test results\n\n\npacman::p_load(tidyverse,  SmartEDA, ggdist, parallelPlot,  ggstatsplot, ggside, plotly, ggpubr)\n\n\n\nIn this exercise, we will be using the exam data\n\nProperty_data &lt;- read_csv(\"data/Property_Price_and_Green_Index.csv\")\n\n\n\nMajority of the variables in the dataset are continuous variables\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 28\n$ `Property Prices`  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9…\n$ Longitude          &lt;dbl&gt; 129.1081, 129.1081, 129.1081, 129.1081, 129.1081, 1…\n$ Latitude           &lt;dbl&gt; 35.21502, 35.21502, 35.21502, 35.21502, 35.21502, 3…\n$ Size               &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.070…\n$ Floor              &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4…\n$ `Highest floor`    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11,…\n$ Units              &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8…\n$ Parking            &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.6…\n$ Heating            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year               &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 201…\n$ `Dist. Green`      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4…\n$ `Dist. Water`      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7…\n$ `Green Index`      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.8678…\n$ `Dist. Subway`     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5…\n$ `Bus Stop`         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, …\n$ `Dist. CBD`        &lt;dbl&gt; 19909.90, 19909.90, 19909.90, 19909.90, 19909.90, 1…\n$ `Top Univ.`        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `High School`      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ `Sex Ratio`        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 9…\n$ Population         &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 33…\n$ `Pop. Density`     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 1…\n$ `Higher Degree`    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 4…\n$ `Young Population` &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 2…\n$ `Median Age`       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.…\n$ `Old Population`   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5…\n$ Spring             &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n$ Fall               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Winter             &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, …\n\n\n\n\nWe check for any columns with missing data using ExpData() from SmartEDA package. There are no missing data in this dataset.\n\nProperty_data %&gt;%\n  ExpData(type=2)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1  Property Prices       numeric    52644             0              0\n2      2        Longitude       numeric    52644             0              0\n3      3         Latitude       numeric    52644             0              0\n4      4             Size       numeric    52644             0              0\n5      5            Floor       numeric    52644             0              0\n6      6    Highest floor       numeric    52644             0              0\n7      7            Units       numeric    52644             0              0\n8      8          Parking       numeric    52644             0              0\n9      9          Heating       numeric    52644             0              0\n10    10             Year       numeric    52644             0              0\n11    11      Dist. Green       numeric    52644             0              0\n12    12      Dist. Water       numeric    52644             0              0\n13    13      Green Index       numeric    52644             0              0\n14    14     Dist. Subway       numeric    52644             0              0\n15    15         Bus Stop       numeric    52644             0              0\n16    16        Dist. CBD       numeric    52644             0              0\n17    17        Top Univ.       numeric    52644             0              0\n18    18      High School       numeric    52644             0              0\n19    19        Sex Ratio       numeric    52644             0              0\n20    20       Population       numeric    52644             0              0\n21    21     Pop. Density       numeric    52644             0              0\n22    22    Higher Degree       numeric    52644             0              0\n23    23 Young Population       numeric    52644             0              0\n24    24       Median Age       numeric    52644             0              0\n25    25   Old Population       numeric    52644             0              0\n26    26           Spring       numeric    52644             0              0\n27    27             Fall       numeric    52644             0              0\n28    28           Winter       numeric    52644             0              0\n   No_of_distinct_values\n1                   2562\n2                   2390\n3                   2386\n4                   5048\n5                     77\n6                     63\n7                    715\n8                    228\n9                      2\n10                    46\n11                  4631\n12                  3059\n13                  2398\n14                  2404\n15                    61\n16                  3942\n17                    27\n18                    31\n19                   382\n20                   380\n21                   381\n22                   380\n23                   381\n24                   112\n25                   381\n26                     2\n27                     2\n28                     2\n\n\n\n\nWe check for presence of duplicated records using duplicated(). The results show that there are no duplicated records.\n\nProperty_data[duplicated(Property_data),]\n\n# A tibble: 0 × 28\n# ℹ 28 variables: Property Prices &lt;dbl&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;,\n#   Size &lt;dbl&gt;, Floor &lt;dbl&gt;, Highest floor &lt;dbl&gt;, Units &lt;dbl&gt;, Parking &lt;dbl&gt;,\n#   Heating &lt;dbl&gt;, Year &lt;dbl&gt;, Dist. Green &lt;dbl&gt;, Dist. Water &lt;dbl&gt;,\n#   Green Index &lt;dbl&gt;, Dist. Subway &lt;dbl&gt;, Bus Stop &lt;dbl&gt;, Dist. CBD &lt;dbl&gt;,\n#   Top Univ. &lt;dbl&gt;, High School &lt;dbl&gt;, Sex Ratio &lt;dbl&gt;, Population &lt;dbl&gt;,\n#   Pop. Density &lt;dbl&gt;, Higher Degree &lt;dbl&gt;, Young Population &lt;dbl&gt;,\n#   Median Age &lt;dbl&gt;, Old Population &lt;dbl&gt;, Spring &lt;dbl&gt;, Fall &lt;dbl&gt;, …\n\n\n\n\n\n\n\n3 Data Wrangling\n\n3.1 Create Seasons Variable\nThe season of transaction is currently encoded as dummy variables in 3 separate columns. Thus, We create a new seasons variable to consolidate the data into a single column, using ifelse statements.\n\nProperty_data$Season &lt;- ifelse(Property_data$Spring==1, \"Spring\", ifelse(Property_data$Fall==1, \"Fall\", ifelse(Property_data$Winter==1, \"Winter\", \"Summer\")))\n\nProperty_data$Season &lt;- factor(Property_data$Season, \n                               levels = c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"), ordered = TRUE )\n\n\n\n3.2 Bin House Size\nAs we have many continuous variables in the dataset, it will be useful to bin some of the continuous variables into categorical variables for further analyses in our project. The distribution of the size of the houses in the dataset is shown in the histogram below. Majority of the houses have size of less than 100m2, with a relatively small number having size greater than 130m2.\n\n\nShow the code\nggplot(Property_data, aes(Size)) +\n  geom_histogram(boundary = 100,\n                 color=\"black\", \n                 fill=\"#7F948F\") +\n  labs(title = \"Frequency of Size\", x=\"Size (m\\u00B2)\")+\n  scale_x_continuous(n.breaks = 20)\n\n\n\n\n\n\n\n\n\nIn a figure from the National Atlas of Korea on gross floor area by housing type, the floor area has been categorised into 5 categories and hence we will bin the house sizes similarly as well: (1) Below 40m2 (2) 40-&lt;60m2(3) 60-&lt;85m2 (4) 85-&lt;130m2 (5) 130m2 and above\n\n\nProperty_data$Size_binned &lt;- ifelse(Property_data$Size&lt;40, \"&lt;40\", ifelse(Property_data$Size&lt;60, \"40-&lt;60\", ifelse(Property_data$Size&lt;85, \"60-&lt;85\", ifelse(Property_data$Size&lt;130, \"85-&lt;130\", \"\\u2265 130\"))))\n\n\n\n3.3 Bin House Floor\nThe distribution of the house floors are shown in the histogram below. Majority of the houses are below 40 floors.\n\n\nShow the code\nggplot(Property_data, aes(Floor)) +\n  geom_histogram(boundary = 100,\n                 color=\"black\", \n                 fill=\"#7F948F\") +\n  labs(title = \"Frequency of Floor\")+\n  scale_x_continuous(n.breaks = 20)\n\n\n\n\n\n\n\n\n\nBased on the graph, we could categorise the floor levels into: (1) Low: &lt;6 (2) Middle: 6-15 (3) Middle-high: 16-25 (4) High: 26-40 (5) Top: &gt;40\n\nProperty_data$Floor_binned &lt;-ifelse(Property_data$Floor&lt;6,\"Low\", ifelse(Property_data$Floor&lt;16,\"Middle\",ifelse(Property_data$Floor&lt;26,\"Middle-high\", ifelse(Property_data$Floor&lt;41,\"High\", \"Top\"))))\n\n\n\n3.3 Bin House Construction Year\nThe year of construction of the houses transacted in the dataset ranges from 1969 to 2019. We will bin the years: (1) 1960s to 1970s: 1969 - 1979 (2) 1980s to 1990s: 1980-1999 (3) 2000s: 2000-2009 (4) 2010s: 2010-2019\n\nProperty_data$Year_binned &lt;-ifelse(Property_data$Year&lt;1980, \"1969-1979\",ifelse(Property_data$Year&lt;2000, \"1980-1999\", ifelse(Property_data$Year&lt;2010, \"2000-2009\", \"2010-2019\")))\n\n\n\n\n4 EDA\n\nDistributions of Environmental Amenities and Local Built Environments\nFirstly, we will generate boxplots and display smoothed histogram to visualise the distributions of the continuous variables in the dataset. In particular, we will look at the distributions of the environmental amenities and local built environments as part of the focus of our project. The environmental factors included in our dataset includes:\n\nDist. Green: Log-transformed network distance to the nearest park, hill, or mountain in meters\nDist. Water: Log-transformed network distance to the nearest river, stream, pond, or seashore in meters\nGreen Index: Degree of street greenness exposed to pedestrians\nDist. Subway: Log-transformed network distance to the nearest subway station in meters\nBus Stop: Number of bus stops within a 400-meter radius of a property\nHigh School: Number of high schools within a 5-km radius of a property\n\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = `Bus Stop`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np2&lt;-ggplot(Property_data, \n       aes(x = `Dist. Green`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np3&lt;-ggplot(Property_data, \n       aes(x = `Dist. Water`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np4&lt;-ggplot(Property_data, \n       aes(x = `Green Index`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\np5&lt;-ggplot(Property_data, \n       aes(x = `Dist. Subway`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np6&lt;-ggplot(Property_data, \n       aes(x = `High School`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n(p1+p2)/(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\n\nFrom the plots above, we can see that:\n\nBus Stop: The distribution is right-skewed, with the majority of houses having fewer than 20 bus stops within a 400-meter radius. While most houses are located near at least one bus stop, which can enhance accessibility, a higher number of bus stops in proximity may also increase exposure to pollution from nearby roads. It would thus be interesting to see how the property prices varies with the number of bus stops in the vicinity later on through CDA.\nDist. Green (Distance to Green Area): The distribution exhibits two peaks, one around 5 and another around 9.6. Therefore, it may be beneficial for further analysis to categorize this variable into two bins based on these two peaks.\nDist. Water (Distance to nearest river, stream, pond, or seashore): The distribution is slightly left skewed.\nGreen Index: The distribution resembles a normal distribution.\nDist. Subway (Distance to Subway Station): This distribution has a concentration around 7.\nHigh School (Distance to High Schools): There are multiple peaks that resemble normal distributions. This suggests that the data might be a mixture of several normal distributions rather than a single one. This suggests that different areas may have distinct clusters of school density. Some possible explanations for this observation may include:\n\nProperties in urban areas or dense cities may have a high density of nearby schools, forming peaks at higher values. In contrast, suburban or rural areas may have fewer nearby schools, creating peaks at lower values.\nSome regions might have policies that cluster multiple schools together to form education hubs.\nAreas with physical barriers (eg rivers, mountains) may have fewer schools nearby due to the land constraints.\n\n\nFor further analysis, we can bin Bus Stop and Dist. Green. Given the large number of peaks for High School (&gt;10), categorizing this variable may oversimplify the data and obscure the underlying patterns, making it less beneficial for analysis.\nFor Bus Stop, we will bin according to this definition: (1) 0-5 (2) 6-10 (3) 11-20 (4) 21-30 (5) &gt;30\nFor Dist. Green, the 2 categories will be defined as: (1) &lt;8 (2) &gt;=8\n\nProperty_data$BusStop_binned &lt;-ifelse(Property_data$`Bus Stop`&lt;=5,\"0-5\", ifelse(Property_data$`Bus Stop`&lt;=10, \"6-10\", ifelse(Property_data$`Bus Stop`&lt;=20, \"11-20\", ifelse(Property_data$`Bus Stop`&lt;=30, \"21-30\", \"&gt;30\"))))\n\nProperty_data$BusStop_binned &lt;- factor(Property_data$BusStop_binned, \n                               levels = c(\"0-5\", \"6-10\", \"11-20\", \"21-30\", \"&gt;30\"), ordered = TRUE )\n\nProperty_data$DistGreen_binned &lt;-ifelse(Property_data$`Dist. Green`&lt;8, \"&lt;8\", \"&gt;=8\")\n\n\n\nShow the code\nProperty_data$Floor_binned &lt;- factor(Property_data$Floor_binned, \n                               levels = c(\"Low\", \"Middle\", \"Middle-high\", \"High\", \"Top\"), ordered = TRUE )\n\np1&lt;-ggplot(Property_data, \n       aes(\n           y = `Property Prices`,\n           fill=`Property Prices`\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\n\n\nProperty Price and Floor\nThe team hopes to enable users to visualise how the distributions of various continuous variables in the dataset may differ if grouped by categorical variables as well. Based on common knowledge and general trends in Singapore housing prices, house prices varies across floor ranges, with houses on higher floors typically fetching higher transaction prices compared to low floors. Therefore, we will examine the relationship between house price and floor range in Busan.\n\n\nShow the code\nProperty_data$Floor_binned &lt;- factor(Property_data$Floor_binned, \n                               levels = c(\"Low\", \"Middle\", \"Middle-high\", \"High\", \"Top\"), ordered = TRUE )\n\np1&lt;-ggplot(Property_data, \n       aes(x = Floor_binned, \n           y = `Property Prices`,\n           fill=Floor_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nBased on the plot, we observe an increasing trend in the median prices of houses in low floors to the top floors. In addition, distribution of the house prices have some resemblance to normal distribution. The median house prices for high and top floors is significantly higher than the median house prices for low floors.\nHowever, confirmatory analysis will be required to examine if the prices are significantly different between the low floor, middle floor and middle-high floors, as the median prices are only slightly different based on the plot above. Thus this could be explored by creating a another chart for visualising statistical test results, as it may be too cluttered if we add in the significance of t-tests into the plot as well, as seen below:\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Floor_binned, \n           y = `Property Prices`,\n           fill=Floor_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  geom_pwc(\n  aes(group = Floor_binned), tip.length = 0,\n  method = \"t_test\", label = \"p.adj.signif\"\n)+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to choose which variables they want to visualise. They can choose either 1 continuous variable only or 1 categorical and 1 continuous variable\nsince plotly does not support stathalfeye, we cannot display summary statistics using interactive plot tool tip. Therefore we can place panels to indicate the summary statistics such as mean/median values for the continuous variable (Overall) instead and an additional table can be included at the side to display summary statistics grouped by the categorical variable\nenable users to filter the range for the continuous variable\n\n\n\nProperty Price and Bus Stops\nNext, we examine if property price varies with number of bus stops in the vicinity.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = BusStop_binned, \n           y = `Property Prices`,\n           fill=BusStop_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"No. of Bus Stops Nearby\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price does not vary with number of bus stops\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nContrary to expectations, there appears to be only slight differences in median housing prices across the binned ranges of bus stops nearby based on the plot. However, this needs to be verified through CDA. This suggests that the proximity to bus stops may not be as influential on housing prices as initially assumed. This could be explained by the diminishing returns of accessibility: while a few bus stops may enhance convenience, the effect might level off or even become a liability as congestion or noise from busier areas increases. Further investigation into additional contextual factors or finer grained analysis might be needed to uncover the true relationship between public transportation access and housing prices.\n\n\nProperty Price and Dist. Green\nNext, we examine if property price varies with distance to greenery.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = DistGreen_binned, \n           y = `Property Prices`,\n           fill=DistGreen_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Distance to Greenery\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price does not vary significantly between DistGreen categories\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nThe median property prices for houses closer to greenery is higher compared to houses further from greenery, as expected. However, the difference appears to be rather small based on the plot above and hence the significance of this difference needs to be verified through CDA. The small difference in housing prices could be due to the following reasons:\n\nValue of proximity to green spaces might not be directly reflected in property prices. Greenery could be valued more highly in specific contexts (such as urban areas), while in other locations, it may not significantly affect housing prices. In addition, some properties may have limited access to greenery, but this could be compensated by other features, such as access to shopping centers, schools, and public transportation, which could reduce the impact of access to greenery on housing prices.\nThere might be a loss of important nuances or subtleties in how different distances to greenery affect housing prices by binning into just 2 categories. A more granular categorization, or examing distance to greenery as a continuous variable, might capture more detailed patterns that are missed when using broad bins.\n\n\n\nHousing Prices and Season of Transaction\nNext, we are interested to find out if the season could affect the housing transaction prices in Korea.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Season, \n           y = `Property Prices`,\n           fill=Season\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Season\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Housing Prices are significantly higher in fall\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nBased on the plot, there is no difference in the median housing prices for transactions that occur during spring, summer and winter. However, median transaction prices during fall is significantly higher compared to other seasons. This could be due to fall season providing a more favorable weather for moving house, Unlike the hot and humid summer or the cold and snowy winter. In addition, families with schooling children may prefer to finalize home purchases in the fall so they can move in before the winter school break. Additionally, some universities and companies also have fall admissions or job relocations, contributing to higher housing demand. The combination of these factors may boost housing demand in fall, leading to upward pressure on prices.\n\n\nYear of Construction & Housing Prices\nLastly, we investigate the relationship between year of construction and housing prices.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Year_binned, \n           y = `Property Prices`,\n           fill=Year_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Year of Construction\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Houses built in 1969-1979 have higher transaction values\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nInterestingly, houses built between 1969 and 1979 have significantly higher transaction prices compared to those built in later decades, while properties from 1980 to 1999 have the lowest median prices. Homes constructed between 2000 and 2009, as well as those from 2010 to 2019, exhibit relatively similar price levels. This trend could be attributed to several factors. Older properties from 1969-1979 may have heritage or architectural significance or hold potential for redevelopment, driving up their market prices as investors anticipate future gains. In contrast, houses built between 1980 and 1999 may fall into a transitional phase where they lack both the charm and of older homes and the modern amenities of newer ones, leading to lower demand and subsequently lower prices. Meanwhile, the homes built after 2000 may have similar features and hence exhibit little variation in housing prices.\n\n\nParallel coordinates plot with histogram\nAs we have many continuous variables in the dataset, it may be useful to explore the use of parallel coordinates plot with histogram.\n\n\nShow the code\ncontinuous_vars&lt;-c(\"Property Prices\", \"Floor\", \"Parking\", \"Year\", \"Dist. Green\", \"Dist. Water\", \"Green Index\", \"Dist. Subway\", \"Bus Stop\", \"High School\" )\ncont_df&lt;-Property_data[,continuous_vars]\n# Normalizing the data to [0, 1] range for each column in the data frame\ndf_normalized &lt;- apply(cont_df, 2, function(x) (x - min(x)) / (max(x) - min(x)))\n\n# Convert the result back to a data frame \ndf_normalized &lt;- as.data.frame(df_normalized)\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(df_normalized[1:3]))\nparallelPlot(df_normalized[1:3],\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\nThe parallel coordinate plot with histogram allows for the simultaneous display of multiple continuous variables, with each variable represented by a vertical axis. Observations are depicted as lines that traverse across the axes By examining the path of each line across the axes, we can easily track how changes in one variable influence others and possibly identify patterns. However, if too many variables are selected, the interactive plot will hang and may be difficult to interpret.\nHence, for our shiny app, we could set a minimum selection of 3 variables and up to 5 variables can be selected.\n\n\n\n5 CDA\n\n5.1 Bivariate Analysis: Continuous Variables\nNext, our team is interested in finding out if there is any significant correlations between the continuous variables in the dataset, particularly between housing prices and the environmental factors.\n\n\nShow the code\ncontinuous_vars&lt;-c(\"Property Prices\", \"Floor\", \"Parking\", \"Dist. Green\", \"Dist. Water\", \"Green Index\", \"Dist. Subway\", \"Bus Stop\", \"High School\", \"Year\")\ncont_df&lt;-Property_data[,continuous_vars]\nggstatsplot::ggcorrmat(\n  data = cont_df\n)\n\n\n\n\n\n\n\n\n\nBased on the correlation heatmap, housing prices do not exhibit a strong linear relationship with any of the environmental factors, as the absolute correlation values remain below 0.5. This suggests that variables such as distance to greenery, water bodies, subway stations, bus stops, and high schools have a more complex influence on property prices that cannot be captured solely through linear correlations. It is possible that these factors interact in a non-linear manner or are influenced by additional contextual variables such as neighborhood desirability or government policies. In contrast, variables like parking space availability, floor level, and construction year show stronger linear relationships with housing prices, indicating that these factors may play a more direct role in determining property values. Several potential reasons could explain this observation:\n\nHousing prices are influenced by combination of multiple factors. Environmental factors like proximity to greenery might not be the primary drivers of price variations.\nThe correlation analysis measures only linear relationships between variables. If the relationship between housing prices and environmental factors is non-linear this may not be captured. For example, being too close to a bus stop or subway station could increase accessibility but also expose residents to noise and pollution, leading to a mixed effect on property values.\nParking space, floor and construction year may have more direct impact on households. Parking space is a practical necessity, particularly in urban areas where parking is limited, making it a strong determinant of property value. Higher floors often offer better views, more privacy, and less noise, making them more desirable. Construction year reflects the age, design, and structural quality of a property. Newer buildings tend to have modern amenities and better infrastructure, making them more expensive.\n\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to select which continuous variables they wish to include in the correlation heatmap. Users have to select a minimum of 3 variables.\nenable users to choose the type of statistical approach: “parametric”, “nonparametric”, “robust” and “bayes”.\nenable users to select the significance level\nenable users to choose the p.adjust.method\n\nTo visualise the relationship and see the p-value, we can incorporate the scatterplot diagram into the shiny application alongside the correlation heatmap. The scatterplot may be useful for complementing the correlation heatmap, as we can possibly add additional panel control in the shiny app to enable users to filter the variable by another categorical variable, or by the range of a continuous variable. This may enable us to better visualise the complex relationships between housing prices and the various factors. Below, we show the scatterplot of property prices and green index.\n\n\nShow the code\nggscatterstats(\n  data = Property_data,\n  x = `Property Prices`,\n  y = `Green Index`,\n  marginal = TRUE,\n  )\n\n\n\n\n\n\n\n\n\nA weak negative correlation is observed between the green index and property prices, suggesting that homes in greener areas tend to be less expensive. This may be because urban areas, where prices are higher, are more developed and have fewer green spaces, while suburban and rural areas have more greenery but lower demand, fewer amenities, and greater distance from city centers. Additionally, green spaces are often far from commercial hubs and transport links, making accessibility a higher priority for buyers. Areas with high Green Index values may also be zoned for parks, nature reserves, or low-density housing, which generally have lower real estate values. In contrast, prime urban properties may have limited greenery but command higher prices due to location advantages. The weak correlation may also be due to the Green Index not capturing green space quality—for instance, large undeveloped areas far from key facilities may add little value, whereas well-maintained parks in prime locations can drive up housing prices.\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to select 2 continuous variables to visualise on the scatterplot\nenable users to choose the type of statistical approach: “parametric”, “nonparametric”, “robust” and “bayes”.\nenable users to filter by range of the continuous variables selected for the scatter plot, enabling the visualization of more complex relationships beyond simple linear trends. Grouping by a categorical variable can reveal additional insights.\n\n\n\n5.2 Bivariate Analysis: Continuous & Categorical Variables\nIn this section, we will do confirmatory analyses for the findings observed in EDA (section 4).\n\nProperty Price and Floor\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Floor_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nBased on the results, we can see that there is indeed an increasing trend of median house price, from low to top floors.\nFor the shiny application, we can add in the following features to enhance interactivity:\n-enable users to choose 1 continuous variable and 1 categorical variable - enable users to choose the type of statistical approach: “parametric”, “nonparametric”, “robust” and “bayes”. - enable users to select which pairwise comparisons to display: (1) “significant” (abbreviation accepted: “s”), (2) “non-significant” (abbreviation accepted: “ns”), (3) “all”. - enable users to choose the p.adjust.method - may be usedul to show a table displaying all the statistic results\n\n\nProperty Price and Bus Stops\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = BusStop_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nThe plot shows that houses with 6-10 bus stops nearby have consistently higher prices compared to those with 0-5 bus stops or more than 10 bus stops. This suggests that having 6-10 bus stops strikes an optimal balance between accessibility and convenience, making these properties more desirable. In contrast, fewer than 6 bus stops may indicate limited public transport options, while more than 10 bus stops might not add further value—potentially due to increased noise, congestion, or diminishing returns in accessibility.\n\n\nProperty Price and Dist. Green\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = DistGreen_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nProperty prices are indeed significantly higher for houses nearer to greenery.\n\n\nHousing Prices and Season of Transaction\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Season, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nApart from fall having higher transaction prices compared to all other seasons, the statistic test also revealed that spring and summer have lower transaction prices than winter, but there are no significant differences in transaction prices in spring and summer.\n\n\nYear of Construction & Housing Prices\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Year_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nHouses built in 1980-1999, 2000-2009 and 2010-2019 folow trend of increasing house prices for newer houses. However, house built in 1969-1979 is significant more valued compared to the newer houses.\n\n\n\n\n6 Storyboard\n\nEDA Module\nWe will create the Boxplot + raincloud plot as shown below, consisting of 2 possible views:\n(1) When one continuous variable is selected only\n\n(2) When 1 continuous variable + 1 categorical variable is selected:\n\nIn addition, we will have a separate tab featuring the parallel coordinate plot with histogram:\n\n\n\nCDA Module\nWe will create 2 tabs for CDA. The first tab will feature the correlation heatmap and scatterplot:\n\nThe second tab will feature the violin-boxplot for statistical tests between groups:\n\n\n\n\n7 Saving the cleaned dataset\nWe will save the final cleaned dataset for use in the shiny app.\n\nwrite.csv(Property_data, \"Property_data_cleaned.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes.html",
    "href": "Teams/Minutes/Meeting_Minutes.html",
    "title": "Meeting Minutes",
    "section": "",
    "text": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline\nProject Meeting 2: Project Proposal\nProject Meeting 3: Project Proposal 2\nProject Meeting 4: Project Poster"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html",
    "href": "Teams/Minutes/Meeting_Minutes2.html",
    "title": "Project Meeting 2: Project Proposal",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 08/03/2025 3.30pm – 5.30pm\nMeeting Agenda:\n\nProject Proposal Storyboard Design\nWebsite Design\nProposal, Data Cleaning & Shiny app Task allocations\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-1-project-proposal-storyboard-design",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-1-project-proposal-storyboard-design",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 1: Project Proposal Storyboard Design",
    "text": "Agenda Item 1: Project Proposal Storyboard Design\nJia Rong suggested that the team could approach data analyses through segregating into univariate, bivariate and multivariate analyses. Jia Rong suggested doing boxplot and histogram for univariate analysis, scatter plot and correlation heat map for bivariate analysis. Xiao Han suggested combining box plot and smoothed histogram together in one plot and doing radar plot and geospatial plot. \nPeng-Wei suggested adding summary of mean and median values above the plot for the univariate analysis. Xiao Han suggested doing cluster analysis and radar chart for multivariate analysis. Jia Rong suggested adding a parallel coordinate plot to visualize the cluster analysis results.  \nPeng-Wei pointed out that our dataset is not very suitable for creating geospatial visualizations as the dataset only has the longitude and latitude coordinates but no region or city data. The team agreed and decided to not focus on geospatial visualizations. Peng Wei also suggested using Canva to draw out the storyboard."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-2-website-design",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-2-website-design",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 2: Website Design",
    "text": "Agenda Item 2: Website Design\nXiao Han suggested that we can use the CSS to decorate the website. \nPeng-Wei proposed using basic CSS for the initial design before adding complex features. \nJia Rong recommended adding a navigation bar at the top with clear labels. \nAll agree that the team could develop a website with a green color scheme to match the visualization designs and the overall theme of the project."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-3-proposal-data-cleaning-shiny-app-task-allocations",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-3-proposal-data-cleaning-shiny-app-task-allocations",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 3: Proposal, Data Cleaning & Shiny app Task allocations",
    "text": "Agenda Item 3: Proposal, Data Cleaning & Shiny app Task allocations\n\n3.1 Proposal\nFor the proposal, all team members agree to continue working on the proposal, with Jia Rong finishing the write-up, Xiao Han working on the storyboard and Peng-Wei working on the website design. \nTo ensure the quality of proposal all team members agree that doing cross check for each other’s part of work and then settle down the whole proposal after checking. \n\n\n3.2 Data Cleaning & Shiny app Task allocations\nAll will work on data cleaning to prepare for the project analyses. The submodules will be split amongst the members as follows: \nXiao Han will handle the cluster analysis components of the Shiny app, including implementation of clustering and visualization of parallel coordinate plot. \nJia Rong will focus on bivariate analysis functionality, developing the correlation heatmap and scatterplot components with interactive selection controls. \nPeng-Wei will be responsible for radar chart implementation and univariate analysis features, including summary statistics, boxplots, and distribution visualizations. \nEach team member will handle data cleaning for their respective modules while maintaining consistent data structures for integration."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-4-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-4-follow-up-action",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 4: Follow-up Action",
    "text": "Agenda Item 4: Follow-up Action\nPeng-Wei needs to upload the proposal on the website before 10 March. \nJia Rong will finalize the proposal write-up by 9 March and share with team members for review. \nXiao Han will complete the storyboard designs using Canva by 9 March for team feedback. \nAll team members will review each other’s work and provide feedback by 10 March to ensure the proposal is ready for submission."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html",
    "href": "Teams/Minutes/Meeting_Minutes4.html",
    "title": "Project Meeting 4: Project Poster",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 29/03/2025 4.00pm – 5.00pm\nMeeting Agenda:\n\nDecide Poster Design and Content\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-1-decide-poster-design-and-content",
    "href": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-1-decide-poster-design-and-content",
    "title": "Project Meeting 4: Project Poster",
    "section": "Agenda Item 1: Decide Poster Design and Content",
    "text": "Agenda Item 1: Decide Poster Design and Content\nThe team agreed to use green color as the theme for the poster, based on the theme of the project. \nPeng Wei suggested adding the QR code to our shiny app in the poster. Jia Rong thinks it depends on whether we can finish setting up the shiny app website before submission of the poster. Xiaohan suggested we can put QR code linked to group website. Finally, all agree we should link QR code directly to shiny app. \nXiao Han suggested only including the latent class analysis results into the poster for the explanatory modelling module, as there is not enough space to include the multiple regression results. The team agreed to pick the more interesting results to showcase in the poster. \nJia Rong suggested that each team member should work on the content for their own modules in the poster and additional work on one of the following sections: (1) Introduction, (2) brief description of the data set and the modules/ methodology for the project and (3) conclusion/future work."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-2-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-2-follow-up-action",
    "title": "Project Meeting 4: Project Poster",
    "section": "Agenda Item 2: Follow-up Action",
    "text": "Agenda Item 2: Follow-up Action\nThe group decided on the following deadlines: poster due Tuesday, individual Shiny app due Thursday. \nThe division of poster is as follows:\n(1) Introduction + EDA/CDA =&gt; Jia Rong\n(2) description of the data set and the methodology + predictive modelling=&gt; Peng Wei\n(3) conclusion/future work + Cluster based on the best LCA results =&gt;Xiaohan\nThe group also decided to merge the individual parts after each member completes their assigned section. Since the data preprocessing varies for each part, it was tentatively decided not to merge the data for now.\nThe team agree that we can do our user guideline after the poster presentation if we can’t finish before the poster presentation."
  }
]